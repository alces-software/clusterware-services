#!/bin/bash -e
#==============================================================================
# Copyright (C) 2017 Stephen F. Norledge and Alces Software Ltd.
#
# This file/package is part of Alces Clusterware.
#
# Alces Clusterware is free software: you can redistribute it and/or
# modify it under the terms of the GNU Affero General Public License
# as published by the Free Software Foundation, either version 3 of
# the License, or (at your option) any later version.
#
# Alces Clusterware is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this package.  If not, see <http://www.gnu.org/licenses/>.
#
# For more information on the Alces Clusterware, please visit:
# https://github.com/alces-software/clusterware
#==============================================================================
if [ "$cw_DEBUG" ]; then
    set -x
fi

setup() {
    local a xdg_config
    IFS=: read -a xdg_config <<< "${XDG_CONFIG_HOME:-$HOME/.config}:${XDG_CONFIG_DIRS:-/etc/xdg}"
    for a in "${xdg_config[@]}"; do
        if [ -e "${a}"/clusterware/config.vars.sh ]; then
            source "${a}"/clusterware/config.vars.sh
            break
        fi
    done
    if [ -z "${cw_ROOT}" ]; then
        echo "$0: unable to locate clusterware configuration"
        exit 1
    fi
    kernel_load
}

join_by() {
  local d=$1
  shift
  echo -n "$1"
  shift
  printf "%s" "${@/#/$d}"
}

main() {
    local image work_dir input_dir output_dir job_uuid launcher entrypoint interactive
    local m mount_src mount_dest mounts
    image="$1"
    work_dir="$2"
    input_dir="${3}"
    output_dir="${4}"

    shift 4

    if [ -z "$work_dir" ]; then
        action_die "please specify work directory."
    fi
    if [ -z "$input_dir" ]; then
        action_die "please specify input directory."
    fi
    if [ -z "$output_dir" ]; then
        action_die "please specify output directory."
    fi

    job_uuid=$(uuid)

    mounts=("$work_dir:$_INTERNAL_ROOT/work" "$input_dir:$_INTERNAL_ROOT/input" "$output_dir:$_INTERNAL_ROOT/output")

    while [[ "$1" == "--mount" ]]; do
      if [ -z "$2" ]; then
        action_die "--mount option requires an argument."
      fi

      mount_src=${2%:*}
      mount_dest=${2#*:}

      if [[ "$mount_src" == "$2" ]]; then
        action_die "Volume mounts should be specified as /path/on/host:/path/in/container."
      fi

      for m in "work" "input" "output"; do
        if [[ "$mount_dest" == "${_INTERNAL_ROOT}/${m}" || "$mount_dest" == "${_INTERNAL_ROOT}/${m}/"* ]]; then
          action_die "Cannot override mount point ${_INTERNAL_ROOT}/${m}."
        fi
      done

      mounts+=("${mount_src}:${mount_dest}")
      shift 2
    done

    run_args=("${@}")

    if [[ "${run_args[0]}" == "--mpi"* ]]; then
      use_mpi="--network gridware-mpi"
      if [[ ${run_args[0]#*=} != ${run_args[0]} ]]; then
        # a number of slaves has been specified
        mpi_slaves=${run_args[0]#*=}
      fi
      run_args=(${run_args[@]:1})
    fi

    if [ "${run_args[0]}" == "--interactive" ]; then
        interactive="-it"
        if [ "${run_args[1]}" ]; then
            launcher=(${run_args[@]:1})
        else
            launcher=(/bin/bash)
        fi
    else
        entrypoint="$(docker inspect ${image} | ${_JQ} -Mrc '.[].Config.Entrypoint | join(" ")')"
        if [ "${run_args[0]}" == "--script" ]; then
            launcher=(${entrypoint} $_INTERNAL_ROOT/work/workload.sh "${run_args[@]:1}")
        elif [ "${run_args[0]}" == "--command" ]; then
            launcher=("${run_args[@]:1}")
        else
            launcher=(${entrypoint} "${run_args[@]}")
        fi
    fi

    cat <<EOF >$work_dir/Dockerfile
FROM ${image}

RUN useradd -m -u $SUDO_UID $SUDO_USER

EOF

   if [[ ! -z "$use_mpi" ]]; then
     # set up SSH key for sshd in container so that all copies of the temp image
     # can log into each other
     mkdir -p $work_dir/ssh
     ssh-keygen -t rsa -f $work_dir/ssh/id_rsa -N ''
     cp $work_dir/ssh/id_rsa.pub $work_dir/ssh/authorized_keys
     cat <<EOF > $work_dir/ssh/config
 Host *
   IdentityFile %d/.ssh/id_rsa
   StrictHostKeyChecking  no
EOF
     cat <<EOF >>$work_dir/Dockerfile
     COPY ssh /home/$SUDO_USER/.ssh
     RUN chown -R $_UID:$_GID /home/$SUDO_USER/.ssh
     RUN for m in \`cat /opt/gridware/etc/defaults\`; do echo "module load \$m" >> /home/$SUDO_USER/.modules; done && \
         chown $_UID:$_GID /home/$SUDO_USER/.modules
EOF
   fi

    docker build -t $job_uuid $work_dir >> $work_dir/docker.log

    if [[ ! -z "$use_mpi" ]]; then
      # Share temporary image with slave nodes
      docker save -o "${cw_GRIDWARE_root:-/opt/gridware}/docker/exports/${job_uuid}" "${job_uuid}"
      chmod 0644 "${cw_GRIDWARE_root:-/opt/gridware}/docker/exports/${job_uuid}"

      for node in $(docker node ls -f "role=worker" | tail -n -1 | cut -d ' ' -f6); do
        # Force each node to import the image!
        ssh $node docker load -i "${cw_GRIDWARE_root:-/opt/gridware}/docker/exports/${job_uuid}"
      done

      # This step currently gives some scary-looking errors that aren't actually errors
      docker service create --network gridware-mpi --replicas ${mpi_slaves:-2} --name ${job_uuid}-service ${job_uuid} --mpi >> $work_dir/docker.log
      # Now wait until all replicas have been created
      while true; do
        replicas=$(docker service ls --filter name=${job_uuid}-service | tail -n -1 | sed -e "s/ \+/ /g" | cut -d' ' -f4)
        ready=${replicas%/*}
        total=${replicas#*/}
        echo "${ready} of ${total} slaves ready"
        if [[ "$ready" == "$total" ]]; then break; fi
        sleep 5
      done

      sleep 5  # wait for containers to get their IP addresses from Docker
      # Assemble a hosts file
      ruby_run <<RUBY

require 'json'

network = JSON.parse(IO.popen("docker network inspect --verbose gridware-mpi").read)[0]

containers = network["Services"]["${job_uuid}-service"]["Tasks"]

File.open("$work_dir/hosts", "w") { |hosts|

  File.open("$work_dir/hostlist", "w") { |hostlist|

    containers.each { |container|
      hostlist.write("#{container["EndpointIP"]}\n")
      hosts.write("#{container["Name"]} #{container["EndpointIP"]}\n")
    }

  }

}

RUBY

    fi

    set +e
    docker run $interactive --rm=true \
           -v $(join_by " -v " "${mounts[@]}") \
           --env "WORK_DIR=$_INTERNAL_ROOT/work" \
           --env "INPUT_DIR=$_INTERNAL_ROOT/input" \
           --env "OUTPUT_DIR=$_INTERNAL_ROOT/output" \
           --user "$_UID:$_GID" \
           $use_mpi \
           $job_uuid \
           "${launcher[@]}"

    if [[ ! -z "$use_mpi" ]]; then
      docker service rm ${job_uuid}-service >> $work_dir/docker.log
      rm "${cw_GRIDWARE_root:-/opt/gridware}/docker/exports/${job_uuid}"
    fi
    docker rmi --force $job_uuid >> $work_dir/docker.log
}

setup
require action
require process
require ruby

if ! process_reexec_sg docker --plain "$@"; then
   action_die "unable to find group: docker"
fi

_INTERNAL_ROOT=/job
_UID=${SUDO_UID:-$UID}
_GID=$(id -g $SUDO_USER)
_JQ="${cw_ROOT}"/opt/jq/bin/jq

main "$@"
